{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing of Raw Text</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import Necessary Libraries</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kleye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import warnings\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from autocorrect import Speller\n",
    "from nltk import download\n",
    "download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"In this book authored by Sohom Ghosh and Dwight Gunning, we shall learnning how to pracess Natueral Language and extract insights from it. The first four chapter will introduce you to the basics of NLP. Later chapters will describe how to deal with complex NLP prajects. If you want to get early access of it, you should book your order now.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apply Tokenization process to the text corpus and print the first 20 tokens</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'book', 'authored', 'by', 'Sohom', 'Ghosh', 'and', 'Dwight', 'Gunning', ',', 'we', 'shall', 'learnning', 'how', 'to', 'pracess', 'Natueral', 'Language', 'and']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sentence)\n",
    "print('{}'.format(words[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apply Spelling Correction on each token and print the initial 20 corrected tokens as well as the corrected text corpus.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = Speller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sohom has been corrected to: Soom\n",
      "Ghosh has been corrected to: Those\n",
      "learnning has been corrected to: learning\n",
      "pracess has been corrected to: process\n",
      "Natueral has been corrected to: Natural\n",
      "NLP has been corrected to: Nap\n",
      "NLP has been corrected to: Nap\n",
      "prajects has been corrected to: projects\n"
     ]
    }
   ],
   "source": [
    "corrected_sentence = \"\"\n",
    "corrected_word_list = []\n",
    "for wd in words:\n",
    "    if wd not in string.punctuation:\n",
    "        wd_c = spell(wd)\n",
    "        if wd_c != wd:\n",
    "            print(\"{} has been corrected to: {}\".format(wd, wd_c))\n",
    "            corrected_sentence = corrected_sentence + \" \" + wd_c\n",
    "            corrected_word_list.append(wd_c)\n",
    "        else:\n",
    "            corrected_sentence = corrected_sentence + \" \" + wd\n",
    "            corrected_word_list.append(wd)\n",
    "    else:\n",
    "        corrected_sentence = corrected_sentence + wd\n",
    "        corrected_word_list.append(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In this book authored by Soom Those and Dwight Gunning, we shall learning how to process Natural Language and extract insights from it. The first four chapter will introduce you to the basics of Nap. Later chapters will describe how to deal with complex Nap projects. If you want to get early access of it, you should book your order now.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'book', 'authored', 'by', 'Soom', 'Those', 'and', 'Dwight', 'Gunning', ',', 'we', 'shall', 'learning', 'how', 'to', 'process', 'Natural', 'Language', 'and']\n"
     ]
    }
   ],
   "source": [
    "print(corrected_word_list[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apply PoS tags to each corrected token and print them.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' ', 'NN'), ('I', 'PRP'), ('n', 'VBP'), (' ', 'JJ'), ('t', 'NN'), ('h', 'NN'), ('i', 'JJ'), ('s', 'VBP'), (' ', 'NN'), ('b', 'NN'), ('o', 'NN'), ('o', 'NN'), ('k', 'NN'), (' ', 'VBZ'), ('a', 'DT'), ('u', 'JJ'), ('t', 'NN'), ('h', 'NN'), ('o', 'JJ'), ('r', 'NN'), ('e', 'NN'), ('d', 'NN'), (' ', 'NN'), ('b', 'NN'), ('y', 'NN'), (' ', 'NNP'), ('S', 'NNP'), ('o', 'MD'), ('o', 'VB'), ('m', 'NN'), (' ', 'NNP'), ('T', 'NNP'), ('h', 'NN'), ('o', 'NN'), ('s', 'NN'), ('e', 'NN'), (' ', 'VBZ'), ('a', 'DT'), ('n', 'JJ'), ('d', 'NN'), (' ', 'NNP'), ('D', 'NNP'), ('w', 'NN'), ('i', 'NN'), ('g', 'VBP'), ('h', 'NN'), ('t', 'NN'), (' ', 'NNP'), ('G', 'NNP'), ('u', 'JJ'), ('n', 'JJ'), ('n', 'NN'), ('i', 'NN'), ('n', 'VBP'), ('g', 'NN'), (',', ','), (' ', 'NNP'), ('w', 'VBZ'), ('e', 'FW'), (' ', 'NNP'), ('s', 'NN'), ('h', 'VBD'), ('a', 'DT'), ('l', 'NN'), ('l', 'NN'), (' ', 'NNP'), ('l', 'NN'), ('e', 'VBZ'), ('a', 'DT'), ('r', 'NN'), ('n', 'NN'), ('i', 'NN'), ('n', 'VBP'), ('g', 'NN'), (' ', 'NN'), ('h', 'NN'), ('o', 'JJ'), ('w', 'NN'), (' ', 'NNP'), ('t', 'NN'), ('o', 'NN'), (' ', 'NNP'), ('p', 'NN'), ('r', 'NN'), ('o', 'IN'), ('c', 'JJ'), ('e', 'NN'), ('s', 'NN'), ('s', 'NN'), (' ', 'NNP'), ('N', 'NNP'), ('a', 'DT'), ('t', 'NN'), ('u', 'JJ'), ('r', 'NN'), ('a', 'DT'), ('l', 'NN'), (' ', 'NNP'), ('L', 'NNP'), ('a', 'DT'), ('n', 'JJ'), ('g', 'NN'), ('u', 'IN'), ('a', 'DT'), ('g', 'NN'), ('e', 'NN'), (' ', 'VBZ'), ('a', 'DT'), ('n', 'JJ'), ('d', 'NN'), (' ', 'NNP'), ('e', 'NN'), ('x', 'NNP'), ('t', 'NN'), ('r', 'VBZ'), ('a', 'DT'), ('c', 'JJ'), ('t', 'NN'), (' ', 'NN'), ('i', 'NN'), ('n', 'VBP'), ('s', 'NN'), ('i', 'NN'), ('g', 'VBP'), ('h', 'NN'), ('t', 'NN'), ('s', 'NN'), (' ', 'NNP'), ('f', 'NN'), ('r', 'NN'), ('o', 'IN'), ('m', 'JJ'), (' ', 'NN'), ('i', 'NN'), ('t', 'NN'), ('.', '.'), (' ', 'CC'), ('T', 'NNP'), ('h', 'VBP'), ('e', 'JJ'), (' ', 'NNP'), ('f', 'NN'), ('i', 'NN'), ('r', 'VBP'), ('s', 'NN'), ('t', 'NN'), (' ', 'NNP'), ('f', 'NN'), ('o', 'NN'), ('u', 'JJ'), ('r', 'NN'), (' ', 'NNP'), ('c', 'VBZ'), ('h', 'VB'), ('a', 'DT'), ('p', 'NN'), ('t', 'NN'), ('e', 'NN'), ('r', 'NN'), (' ', 'NNP'), ('w', 'NN'), ('i', 'NN'), ('l', 'VBP'), ('l', 'NN'), (' ', 'NN'), ('i', 'NN'), ('n', 'VBP'), ('t', 'NN'), ('r', 'NN'), ('o', 'NN'), ('d', 'NN'), ('u', 'JJ'), ('c', 'NN'), ('e', 'NN'), (' ', 'NNP'), ('y', 'NNP'), ('o', 'MD'), ('u', 'VB'), (' ', 'NNP'), ('t', 'NN'), ('o', 'NN'), (' ', 'NNP'), ('t', 'NN'), ('h', 'NN'), ('e', 'NN'), (' ', 'NNP'), ('b', 'VBZ'), ('a', 'DT'), ('s', 'NN'), ('i', 'NN'), ('c', 'VBP'), ('s', 'NN'), (' ', 'NNP'), ('o', 'VBZ'), ('f', 'JJ'), (' ', 'NNP'), ('N', 'NNP'), ('a', 'DT'), ('p', 'NN'), ('.', '.'), (' ', 'VB'), ('L', 'NNP'), ('a', 'DT'), ('t', 'NN'), ('e', 'NN'), ('r', 'NN'), (' ', 'NNP'), ('c', 'VBZ'), ('h', 'VB'), ('a', 'DT'), ('p', 'NN'), ('t', 'NN'), ('e', 'NN'), ('r', 'NN'), ('s', 'NN'), (' ', 'NNP'), ('w', 'NN'), ('i', 'NN'), ('l', 'VBP'), ('l', 'NN'), (' ', 'NNP'), ('d', 'NN'), ('e', 'NN'), ('s', 'NN'), ('c', 'VBP'), ('r', 'NN'), ('i', 'NN'), ('b', 'VBP'), ('e', 'NN'), (' ', 'NN'), ('h', 'NN'), ('o', 'JJ'), ('w', 'NN'), (' ', 'NNP'), ('t', 'NN'), ('o', 'NN'), (' ', 'NNP'), ('d', 'NN'), ('e', 'VBZ'), ('a', 'DT'), ('l', 'NN'), (' ', 'NNP'), ('w', 'NN'), ('i', 'NN'), ('t', 'VBP'), ('h', 'NN'), (' ', 'NNP'), ('c', 'VBZ'), ('o', 'JJ'), ('m', 'NN'), ('p', 'NN'), ('l', 'NN'), ('e', 'NN'), ('x', 'NNP'), (' ', 'NNP'), ('N', 'NNP'), ('a', 'DT'), ('p', 'NN'), (' ', 'NN'), ('p', 'NN'), ('r', 'NN'), ('o', 'IN'), ('j', 'NN'), ('e', 'NN'), ('c', 'VBP'), ('t', 'NN'), ('s', 'NN'), ('.', '.'), (' ', 'NN'), ('I', 'PRP'), ('f', 'VBP'), (' ', 'JJ'), ('y', 'NN'), ('o', 'NN'), ('u', 'JJ'), (' ', 'NNP'), ('w', 'VBD'), ('a', 'DT'), ('n', 'JJ'), ('t', 'NN'), (' ', 'NNP'), ('t', 'NN'), ('o', 'NN'), (' ', 'NNP'), ('g', 'NN'), ('e', 'NN'), ('t', 'NN'), (' ', 'NNP'), ('e', 'VBZ'), ('a', 'DT'), ('r', 'NN'), ('l', 'NN'), ('y', 'NN'), (' ', 'VBZ'), ('a', 'DT'), ('c', 'JJ'), ('c', 'NN'), ('e', 'NN'), ('s', 'JJ'), ('s', 'NN'), (' ', 'NNP'), ('o', 'VBZ'), ('f', 'JJ'), (' ', 'NN'), ('i', 'NN'), ('t', 'NN'), (',', ','), (' ', 'NNP'), ('y', 'NNP'), ('o', 'MD'), ('u', 'VB'), (' ', 'NNP'), ('s', 'NN'), ('h', 'NN'), ('o', 'JJ'), ('u', 'JJ'), ('l', 'NN'), ('d', 'NN'), (' ', 'NN'), ('b', 'NN'), ('o', 'NN'), ('o', 'NN'), ('k', 'NN'), (' ', 'NNP'), ('y', 'NN'), ('o', 'NN'), ('u', 'JJ'), ('r', 'NN'), (' ', 'NNP'), ('o', 'VBZ'), ('r', 'NN'), ('d', 'NN'), ('e', 'NN'), ('r', 'NN'), (' ', 'NNP'), ('n', 'CC'), ('o', 'JJ'), ('w', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "pos_sentence = pos_tag(corrected_sentence)\n",
    "print(\"{}\".format(pos_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Remove stop words from the corrected token list and print the initial 20 tokens.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_words = word_tokenize(corrected_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'this', 'book', 'authored', 'by', 'Soom', 'Those', 'and', 'Dwight', 'Gunning', ',', 'we', 'shall', 'learning', 'how', 'to', 'process', 'Natural', 'Language', 'and', 'extract', 'insights', 'from', 'it', '.', 'The', 'first', 'four', 'chapter', 'will', 'introduce', 'you', 'to', 'the', 'basics', 'of', 'Nap', '.', 'Later', 'chapters', 'will', 'describe', 'how', 'to', 'deal', 'with', 'complex', 'Nap', 'projects', '.', 'If', 'you', 'want', 'to', 'get', 'early', 'access', 'of', 'it', ',', 'you', 'should', 'book', 'your', 'order', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"{}\".format(sentence_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('English')\n",
    "print('{}'.format(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'book', 'authored', 'Soom', 'Those', 'Dwight', 'Gunning', ',', 'shall', 'learning', 'process', 'Natural', 'Language', 'extract', 'insights', '.', 'The', 'first', 'four', 'chapter']\n"
     ]
    }
   ],
   "source": [
    "sentence_no_stops = [word for word in sentence_words if word not in stop_words]\n",
    "print(\"{}\".format(sentence_no_stops[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Apply stemming and lemmatization to the corrected token list and the print initial 20 tokens.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Stemming</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'book', 'author', 'soom', 'those', 'dwight', 'gun', ',', 'shall', 'learn', 'process', 'natur', 'languag', 'extract', 'insight', '.', 'the', 'first', 'four', 'chapter']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "sentence_stem_words = [stemmer.stem(word) for word in sentence_no_stops]\n",
    "print('{}'.format(sentence_stem_words[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lemmatization</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'book', 'authored', 'Soom', 'Those', 'Dwight', 'Gunning', ',', 'shall', 'learning', 'process', 'Natural', 'Language', 'extract', 'insight', '.', 'The', 'first', 'four', 'chapter']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "sentence_lemm_words = [lemmatizer.lemmatize(word) for word in sentence_no_stops]\n",
    "print('{}'.format(sentence_lemm_words[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Detect the sentence boundaries in the given text corpus and print the total number of sentences.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' In this book authored by Soom Those and Dwight Gunning, we shall learning how to process Natural Language and extract insights from it.',\n",
       " 'The first four chapter will introduce you to the basics of Nap.',\n",
       " 'Later chapters will describe how to deal with complex Nap projects.',\n",
       " 'If you want to get early access of it, you should book your order now.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corrected_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' In this book authored by Soom Those and Dwight Gunning, we shall learning how to process Natural Language and extract insights from it.', 'The first four chapter will introduce you to the basics of Nap.', 'Later chapters will describe how to deal with complex Nap projects.', 'If you want to get early access of it, you should book your order now.'] 4\n"
     ]
    }
   ],
   "source": [
    "print('{} {}'.format(sent_tokenize(corrected_sentence), len(sent_tokenize(corrected_sentence))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
